{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.seterr(over='raise', invalid='raise', divide='raise');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../train.csv' # TODO: download train data and supply path here \n",
    "y, X, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "X0, X1, X2, indices0, indices1, indices2 = separate_data(X)\n",
    "y0 = y[indices0]\n",
    "y1 = y[indices1]\n",
    "y2 = y[indices2]\n",
    "overall_acc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Ridge and logistic Regression, Logistic regression outperformed ridge in group 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from Final_CrossValidation import *\n",
    "k_fold=4\n",
    "lamb=[0,0.01,0.02,0.03]\n",
    "degree_0 = 2\n",
    "gamma=0.0001\n",
    "max_iters=100\n",
    "current_y=np.ones(len(y0))\n",
    "current_y[np.where(y0==0)]=-1\n",
    "phi_X0=build_poly_cos_sin_poly(X0, degree_0, np.array(range(1,19)))\n",
    "k_indices = build_k_indices(current_y, k_fold, 1)\n",
    "w=cross_validation_demo_ridge_regression(current_y,phi_X0,k_fold,lamb,seed=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=43154.941985154444\n",
      "Current iteration=100, the loss=28430.95575722621\n",
      "Current iteration=200, the loss=27926.56358211617\n",
      "Current iteration=300, the loss=27626.502312901775\n",
      "Current iteration=400, the loss=27402.698054253764\n",
      "Current iteration=500, the loss=27225.790213453503\n",
      "Current iteration=600, the loss=27081.846805867528\n",
      "Current iteration=700, the loss=26962.335051316943\n",
      "Current iteration=800, the loss=26861.487668566555\n",
      "Current iteration=900, the loss=26775.22110930312\n",
      "Current iteration=1000, the loss=26700.560403318603\n",
      "Current iteration=1100, the loss=26635.290758199015\n",
      "Current iteration=1200, the loss=26577.73350532007\n",
      "Current iteration=1300, the loss=26526.596451023142\n",
      "Current iteration=1400, the loss=26480.870553027053\n",
      "Current iteration=1500, the loss=26439.756674072225\n",
      "Current iteration=1600, the loss=26402.61288879822\n",
      "Current iteration=1700, the loss=26368.91642004464\n",
      "Current iteration=1800, the loss=26338.236061464846\n",
      "Current iteration=1900, the loss=26310.21199794578\n",
      "Current iteration=2000, the loss=26284.540819041915\n",
      "Current iteration=2100, the loss=26260.96423315547\n",
      "Current iteration=2200, the loss=26239.260470040375\n",
      "Current iteration=2300, the loss=26219.237661764502\n",
      "Current iteration=2400, the loss=26200.72869172431\n",
      "Current iteration=2500, the loss=26183.587140731062\n",
      "Current iteration=2600, the loss=26167.68405944627\n",
      "Current iteration=2700, the loss=26152.90536900593\n",
      "Current iteration=2800, the loss=26139.149744141214\n",
      "Current iteration=2900, the loss=26126.326870999903\n",
      "Current iteration=3000, the loss=26114.3559992731\n",
      "Current iteration=3100, the loss=26103.16472810994\n",
      "Current iteration=3200, the loss=26092.687979803788\n",
      "Current iteration=3300, the loss=26082.8671258845\n",
      "Current iteration=3400, the loss=26073.649238140242\n",
      "Current iteration=3500, the loss=26064.986442986952\n",
      "Current iteration=3600, the loss=26056.835362052003\n",
      "Current iteration=3700, the loss=26049.156625228046\n",
      "Current iteration=3800, the loss=26041.9144450644\n",
      "Current iteration=3900, the loss=26035.07624339398\n",
      "Current iteration=4000, the loss=26028.612322691493\n",
      "Current iteration=4100, the loss=26022.49557592613\n",
      "Current iteration=4200, the loss=26016.701229689512\n",
      "Current iteration=4300, the loss=26011.20661620213\n",
      "Current iteration=4400, the loss=26005.990970472747\n",
      "Current iteration=4500, the loss=26001.035249438355\n",
      "Current iteration=4600, the loss=25996.3219703695\n",
      "Current iteration=4700, the loss=25991.83506620825\n",
      "Current iteration=4800, the loss=25987.559755827082\n",
      "Current iteration=4900, the loss=25983.48242746836\n",
      "Current iteration=5000, the loss=25979.59053385446\n",
      "Current iteration=5100, the loss=25975.872497655735\n",
      "Current iteration=5200, the loss=25972.31762617207\n",
      "Current iteration=5300, the loss=25968.916034229245\n",
      "Current iteration=5400, the loss=25965.65872712805\n",
      "Current iteration=5500, the loss=25962.54148836209\n",
      "Current iteration=5600, the loss=25959.555815075488\n",
      "Current iteration=5700, the loss=25956.692824673817\n",
      "Current iteration=5800, the loss=25953.944791646427\n",
      "Current iteration=5900, the loss=25951.30477553644\n",
      "Current iteration=6000, the loss=25948.766471872546\n",
      "Current iteration=6100, the loss=25946.32411490929\n",
      "Current iteration=6200, the loss=25943.972405243447\n",
      "Current iteration=6300, the loss=25941.706452933773\n",
      "Current iteration=6400, the loss=25939.52173138864\n",
      "Current iteration=6500, the loss=25937.414039145635\n",
      "Current iteration=6600, the loss=25935.379467633455\n",
      "Current iteration=6700, the loss=25933.41437358603\n",
      "Current iteration=6800, the loss=25931.515355154064\n",
      "Current iteration=6900, the loss=25929.67923101072\n",
      "Current iteration=7000, the loss=25927.90302192312\n",
      "Current iteration=7100, the loss=25926.183934383815\n",
      "Current iteration=7200, the loss=25924.5193459851\n",
      "Current iteration=7300, the loss=25922.90679228453\n",
      "Current iteration=7400, the loss=25921.34398186949\n",
      "Current iteration=7500, the loss=25919.828687897425\n",
      "Current iteration=7600, the loss=25918.358856345014\n",
      "Current iteration=7700, the loss=25916.932559325447\n",
      "Current iteration=7800, the loss=25915.547965106773\n",
      "Current iteration=7900, the loss=25914.203343219233\n",
      "Current iteration=8000, the loss=25912.897057761496\n",
      "Current iteration=8100, the loss=25911.627561636655\n",
      "Current iteration=8200, the loss=25910.393391466732\n",
      "Current iteration=8300, the loss=25909.193158643997\n",
      "Current iteration=8400, the loss=25908.025544104457\n",
      "Current iteration=8500, the loss=25906.88929845149\n",
      "Current iteration=8600, the loss=25905.783236099232\n",
      "Current iteration=8700, the loss=25904.706230934007\n",
      "Current iteration=8800, the loss=25903.657212681854\n",
      "Current iteration=8900, the loss=25902.63516353775\n",
      "Current iteration=9000, the loss=25901.63911503021\n",
      "Current iteration=9100, the loss=25900.668145102085\n",
      "Current iteration=9200, the loss=25899.721375390473\n",
      "Current iteration=9300, the loss=25898.797968690113\n",
      "Current iteration=9400, the loss=25897.897126586082\n",
      "Current iteration=9500, the loss=25897.018087243072\n",
      "Current iteration=9600, the loss=25896.160123339407\n",
      "Current iteration=9700, the loss=25895.322540135217\n",
      "Current iteration=9800, the loss=25894.50467366489\n",
      "Current iteration=9900, the loss=25893.70588904498\n",
      "Current iteration=10000, the loss=25892.925578889182\n",
      "Current iteration=10100, the loss=25892.16316182304\n",
      "Current iteration=10200, the loss=25891.418081091113\n",
      "Current iteration=10300, the loss=25890.689803250596\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ccd841e14c22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mtraining_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly_cos_sin_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mw_star\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nadeen\\Desktop\\finalized_submission_ml\\ml-group78-submission\\Project_scripts\\implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, gamma, max_iters)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;31m# start the logistic regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# get loss and updated w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nadeen\\Desktop\\finalized_submission_ml\\ml-group78-submission\\Project_scripts\\implementations.py\u001b[0m in \u001b[0;36mcalculate_gradient_log_likelihood\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_gradient_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[1;34m\"\"\"compute the gradient of negative log likelihood.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;31m# stochastic gradient descent helpers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from implementations import *\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters0 = 13000\n",
    "gamma0 = 1e-6\n",
    "lambda_0 = 0\n",
    "degree_0 = 2\n",
    "batch_size0 = 100\n",
    "k = 4\n",
    "current_y = y0\n",
    "current_X = X0\n",
    "k_indices = build_k_indices(current_y, k, 1)\n",
    "\n",
    "loss_tr = []\n",
    "loss_te = []\n",
    "classification_acc = []\n",
    "\n",
    "for k_ in range(k):\n",
    "\n",
    "    test_indices = k_indices[k_]\n",
    "    test_y, test_x = (current_y[test_indices], current_X[test_indices])\n",
    "    test_x = build_poly_cos_sin_poly(test_x, degree_0, np.array(range(1,19)))\n",
    "\n",
    "    training_indices = np.ravel(np.delete(k_indices, k_, axis=0))\n",
    "    training_y, training_x = (current_y[training_indices], current_X[training_indices])\n",
    "    training_x = build_poly_cos_sin_poly(training_x, degree_0, np.array(range(1,19)))\n",
    "\n",
    "    w_star = reg_logistic_regression(training_y, training_x, lambda_0, gamma0, max_iters0)\n",
    "   \n",
    "    \n",
    "    # compute classification accuracy\n",
    "    y_pred = predict_labels_log_regression(w_star, test_x)\n",
    "    \n",
    "    test_y[test_y == 0] = -1\n",
    "    classification_acc.append(np.mean(np.abs(test_y + y_pred) / 2))\n",
    "    test_y[test_y == -1] = 0\n",
    "\n",
    "    loss_tr.append(calculate_loss_log_likelihood(training_y, training_x, w_star))\n",
    "    loss_te.append(calculate_loss_log_likelihood(test_y, test_x, w_star))\n",
    "        \n",
    "print(\"Training error: {tr}\\nTest error: {te}\\nClassification accuracy: {cl}\".format(tr=np.mean(loss_tr), te=np.mean(loss_te), cl=np.mean(classification_acc)))\n",
    "overall_acc.append(np.mean(classification_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression outperformed Logistic for group 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss=0.28310022968560256\n",
      "The loss=0.2830360336727703\n",
      "The loss=0.2812760401972303\n",
      "The loss=0.2835648428878991\n",
      "MSEtr=[ 0.28274429],MSEte=[ 0.28436645], MSEvartr=[  7.60217989e-07], MSEvarte=[  7.13341221e-06], Accuracy=[ 0.80652275]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  5.62096839e-02,  -3.48886180e-01,   1.10341880e+00,\n",
       "        -2.31044834e-02,  -1.31999765e+05,  -1.31999803e+05,\n",
       "         1.24569815e-03,   7.07722682e-01,  -1.24406380e+00,\n",
       "        -5.01593174e-02,  -6.25311033e-02,  -3.03065683e-01,\n",
       "         1.95850421e-02,  -1.90179463e+05,  -1.90179582e+05,\n",
       "        -7.58607305e-04,   1.08969752e-01,   1.04654660e-01,\n",
       "        -3.22352370e-01,  -3.05574322e-01,   1.04340483e-01,\n",
       "         4.22446707e-02,   5.22595765e+04,   5.22595178e+04,\n",
       "        -1.61453636e-03,   4.37161523e-01,  -3.29055669e-01,\n",
       "        -1.01213387e-01,  -7.91003372e-02,   6.99681811e-02,\n",
       "         6.53675977e-02,   9.82711259e+04,   9.82710672e+04,\n",
       "        -3.63752672e-03,   6.79492962e-03,   2.44314758e-02,\n",
       "         9.68483564e-01,   5.28730924e-01,  -8.90146474e-01,\n",
       "         1.00563123e-01,   2.09805956e+05,   2.09806061e+05,\n",
       "        -7.07560915e-02,   4.39609944e-02,  -7.11723911e-02,\n",
       "         5.63161964e-02,   1.00374713e-01,  -6.08559035e-02,\n",
       "        -3.45983317e-03,  -2.53798386e+04,  -2.53799012e+04,\n",
       "         1.08537733e-04,  -5.94686294e-02,   6.64281187e-02,\n",
       "         4.19505808e-02,  -2.72449106e-02,  -2.12265177e-02,\n",
       "        -7.11663101e-03,   3.31155004e+05,   3.31154977e+05,\n",
       "         8.08548041e-05,   2.33667879e-02,   5.27260285e-02,\n",
       "         1.52961795e+02,  -2.07477071e+02,  -1.52906488e+02,\n",
       "        -6.95026676e+01,  -1.20074224e+05,  -1.20119834e+05,\n",
       "        -2.21036745e+01,  -7.75136335e+00,  -3.54024193e+00,\n",
       "         2.46602232e-01,   2.25668698e-02,   7.82366407e-02,\n",
       "        -2.38463486e-02,  -4.19497195e+04,  -4.19497442e+04,\n",
       "         6.57363079e-04,  -5.15185125e-03,   9.17784601e-03,\n",
       "         2.47765465e+00,  -3.26680260e+00,  -2.45057794e+00,\n",
       "        -1.06686364e+00,  -3.75107047e+05,  -3.75108193e+05,\n",
       "        -3.27024285e-01,  -4.32106845e-01,  -1.60398540e-01,\n",
       "        -5.97473924e+00,  -1.47490627e+01,   5.93348468e+00,\n",
       "        -4.69809323e+00,  -1.88656010e+05,  -1.88660189e+05,\n",
       "         8.15419902e-01,  -1.03258787e+00,   2.82049774e-01,\n",
       "         7.01103117e-02,   4.01199899e-02,  -1.20216910e-02,\n",
       "        -1.17643628e-02,  -1.75826514e+04,  -1.75826819e+04,\n",
       "         6.04191438e-04,   5.55471248e-03,   2.28953610e-02,\n",
       "         3.74147134e-01,   6.64977729e+00,  -3.79619744e-01,\n",
       "         2.04773205e+00,   2.90981006e+04,   2.91001941e+04,\n",
       "        -5.14610611e-02,   6.48489771e-01,  -1.40627911e-02,\n",
       "         2.03491123e+00,  -1.52291709e+01,  -2.02985408e+00,\n",
       "        -4.85773609e+00,   2.66006036e+05,   2.66001745e+05,\n",
       "        -2.87068928e-01,  -1.07369151e+00,  -5.92975715e-02,\n",
       "        -1.12641192e-02,  -5.82923477e-02,  -4.20264886e-02,\n",
       "         1.86742595e-04,   9.08350340e+04,   9.08350149e+04,\n",
       "        -2.03121988e-05,   1.21360179e-01,   5.17834964e-02,\n",
       "         1.64381100e+00,   3.90533469e-01,  -1.64235437e+00,\n",
       "         1.20522705e-01,  -1.52504198e+05,  -1.52504069e+05,\n",
       "        -2.26072866e-01,   3.47104224e-02,  -5.68765641e-02,\n",
       "        -7.56314308e-02,   9.30699965e-03,   5.90967876e-02,\n",
       "         5.87619649e-03,   3.31019107e+05,   3.31019047e+05,\n",
       "        -6.90274073e-04,   1.52927647e-03,   3.10795271e-02,\n",
       "         4.77536436e+03,   2.62023140e+04,  -1.04508595e+04,\n",
       "         1.45237082e+03,   1.60932841e+05,   1.64288387e+05,\n",
       "        -2.20325300e+02,  -9.57311117e+03,   6.84483811e+03,\n",
       "         2.67281794e-01,  -4.94487604e-01,  -2.62416247e-01,\n",
       "        -5.46289716e-02,  -1.96280644e+05,  -1.96280681e+05,\n",
       "        -3.22990566e-02,   7.51289694e-03,  -3.99745497e-02,\n",
       "         1.37149406e+00,  -8.60337635e+00,  -1.37674105e+00,\n",
       "        -2.73409583e+00,  -1.15803173e+05,  -1.15805655e+05,\n",
       "        -1.92751490e-01,  -6.44202261e-01,  -3.61228283e-02,\n",
       "        -4.77508910e+03,  -2.62022636e+04,   1.04507664e+04,\n",
       "        -1.45243258e+03,  -1.36140361e+04,  -1.69695613e+04,\n",
       "         2.20328405e+02,   9.57314019e+03,  -6.84484389e+03])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold=4\n",
    "lamb=[0]\n",
    "degree_1 = 3\n",
    "gamma=0.0001\n",
    "max_iters=100\n",
    "from Final_CrossValidation import *\n",
    "from implementations import *\n",
    "current_y=np.ones(len(y1))\n",
    "current_y[np.where(y1==0)]=-1\n",
    "phi_X1=build_poly_cos_sin_poly(X1, degree_1, np.array(range(1,23)))\n",
    "k_indices = build_k_indices(current_y, k_fold, 1)\n",
    "cross_validation_demo_ridge_regression(current_y,phi_X1,k_fold,lamb,seed=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=39461.46257573037\n",
      "Current iteration=100, the loss=32988.911946371394\n",
      "Current iteration=200, the loss=30803.119110640833\n",
      "Current iteration=300, the loss=29612.156736426565\n",
      "Current iteration=400, the loss=29086.19546077023\n",
      "Current iteration=500, the loss=28535.66319010391\n",
      "Current iteration=600, the loss=28104.442704529498\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9515a75602ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mtraining_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly_cos_sin_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m23\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mw_star\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# compute classification accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nadeen\\Desktop\\finalized_submission_ml\\ml-group78-submission\\Project_scripts\\implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, gamma, max_iters)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;31m# start the logistic regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# get loss and updated w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nadeen\\Desktop\\finalized_submission_ml\\ml-group78-submission\\Project_scripts\\implementations.py\u001b[0m in \u001b[0;36mcalculate_gradient_log_likelihood\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_gradient_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[1;34m\"\"\"compute the gradient of negative log likelihood.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;31m# stochastic gradient descent helpers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from implementations import *\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters_1 = 25000\n",
    "gamma_1 = 0.9e-7\n",
    "lambda_1 = 0\n",
    "degree_1 = 3\n",
    "batch_size1 = 100\n",
    "k = 4\n",
    "current_y = y1\n",
    "current_X = X1\n",
    "k_indices = build_k_indices(current_y, k, 1)\n",
    "\n",
    "loss_tr = []\n",
    "loss_te = []\n",
    "classification_acc = []\n",
    "for k_ in range(k):\n",
    "\n",
    "    test_indices = k_indices[k_]\n",
    "    test_y, test_x = (current_y[test_indices], current_X[test_indices])\n",
    "    test_x = build_poly_cos_sin_poly(test_x, degree_1, np.array(range(1,23)))\n",
    "\n",
    "    training_indices = np.ravel(np.delete(k_indices, k_, axis=0))\n",
    "    training_y, training_x = (current_y[training_indices], current_X[training_indices])\n",
    "    training_x = build_poly_cos_sin_poly(training_x, degree_1, np.array(range(1,23)))\n",
    "\n",
    "    w_star = reg_logistic_regression(training_y, training_x, lambda_1, gamma_1, max_iters_1)\n",
    "    \n",
    "    # compute classification accuracy\n",
    "    y_pred = predict_labels_log_regression(w_star, test_x)\n",
    "    \n",
    "    test_y[test_y == 0] = -1\n",
    "    classification_acc.append(np.mean(np.abs(test_y + y_pred) / 2))\n",
    "    test_y[test_y == -1] = 0\n",
    "\n",
    "    loss_tr.append(calculate_loss_log_likelihood(training_y, training_x, w_star))\n",
    "    loss_te.append(calculate_loss_log_likelihood(test_y, test_x, w_star))\n",
    "        \n",
    "print(\"Training error: {tr}\\nTest error: {te}\\nClassification accuracy: {cl}\".format(tr=np.mean(loss_tr), te=np.mean(loss_te), cl=np.mean(classification_acc)))\n",
    "overall_acc.append(np.mean(classification_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression outperformed Logistic Regression for group 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss=0.25480653066052217\n",
      "The loss=0.2552124523800644\n",
      "The loss=0.25673966651773655\n",
      "The loss=0.2555080518409572\n",
      "The loss=0.3170013199837799\n",
      "The loss=0.31700546835641685\n",
      "The loss=0.3188807544817881\n",
      "The loss=0.31728652777560756\n",
      "The loss=0.3361904706029165\n",
      "The loss=0.3362422521279789\n",
      "The loss=0.3381704964428979\n",
      "The loss=0.33670230214338054\n",
      "MSEtr=[ 0.25556668  0.31754352  0.33682638],MSEte=[ 0.25770984  0.3184833   0.337776  ], MSEvartr=[  5.20659714e-07   6.09430373e-07   6.41907615e-07], MSEvarte=[  3.71565974e-06   5.83546487e-06   5.23143721e-06], Accuracy=[ 0.83264406  0.7892611   0.77193273]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  3.65633748e-02,   2.23418959e-02,   3.95068247e-02,\n",
       "        -4.17755035e-02,   2.67020126e-02,  -3.00262583e-02,\n",
       "         3.14665695e-03,   3.75161979e-02,   8.95728130e-03,\n",
       "        -3.47959048e-02,   7.20363070e-03,  -2.52424980e-02,\n",
       "        -1.69419467e-02,   3.09109335e-03,  -6.41533902e-03,\n",
       "         1.09086936e-03,   5.19388365e-03,  -1.72553822e-02,\n",
       "         2.17235401e-02,   1.81676946e-02,   2.42403687e-02,\n",
       "        -1.62226900e-02,   1.79860940e-02,  -2.13103397e-02,\n",
       "         5.65012214e-04,   2.52039547e-02,   3.74374670e-03,\n",
       "         2.23530327e-02,  -8.56228123e-03,   1.60347192e-02,\n",
       "         9.94295217e-03,  -4.07197875e-03,   7.47733092e-04,\n",
       "         5.67706737e-03,  -6.22903385e-03,   9.84631344e-03,\n",
       "         1.88155228e-02,  -1.23690843e-02,   1.65137169e-02,\n",
       "         2.01945105e-02,  -1.39816472e-02,   1.06574015e-02,\n",
       "         1.23681473e-02,  -1.35655252e-02,   7.61760315e-03,\n",
       "         1.82682767e-02,  -1.09269394e-02,   1.36683963e-02,\n",
       "         1.52592573e-02,  -8.82142742e-03,   5.49718176e-03,\n",
       "        -1.74592321e-03,  -1.06662209e-02,   5.82201027e-03,\n",
       "        -1.56415628e-02,  -9.11511526e-03,  -1.39635419e-02,\n",
       "         1.09725505e-02,  -9.64140352e-03,   6.31715786e-03,\n",
       "         1.63489089e-03,  -1.02285992e-02,  -8.27169139e-03,\n",
       "         1.13745501e-02,   1.05905421e-02,   8.68853605e-03,\n",
       "        -3.70960368e-02,   7.71478377e-03,  -1.10390294e-02,\n",
       "         1.59088896e-02,   9.09898522e-03,   8.46055577e-03,\n",
       "        -2.80785795e-02,  -2.50693435e-03,  -2.34490284e-02,\n",
       "         3.22659416e-03,  -6.43880078e-03,   3.11455512e-03,\n",
       "         4.14539654e-04,  -6.65546531e-03,  -1.42871809e-02,\n",
       "        -2.00551279e-02,  -2.48013538e-03,  -1.35422110e-02,\n",
       "        -5.71298670e-03,  -6.62965222e-03,   3.30540656e-03,\n",
       "         2.50044288e-04,  -5.66923811e-03,  -8.21522932e-03,\n",
       "         2.59749954e-02,  -1.30612978e-03,   2.25698344e-02,\n",
       "        -5.11331876e-03,  -1.78370858e-03,  -1.54053709e-03,\n",
       "         2.17066699e-02,  -2.51762562e-03,   1.17602738e-02,\n",
       "         2.75099883e-02,  -6.26459074e-03,   2.24871618e-02,\n",
       "         7.12590349e-03,  -6.32714600e-03,   3.00290034e-03,\n",
       "         3.22784885e-02,  -5.88745419e-03,   1.60208878e-02,\n",
       "         3.28654600e-02,  -4.99618908e-03,   2.60761848e-02,\n",
       "         4.82257272e-03,  -4.85728809e-04,  -2.83851685e-03,\n",
       "        -6.04113071e-04,   3.21972993e-04,   1.34847678e-02,\n",
       "        -1.65181948e-03,   5.40955194e-03,  -1.38842274e-03,\n",
       "        -2.27965907e-02,   3.89219652e-03,  -7.21644218e-03,\n",
       "        -1.63347691e-03,   4.30721811e-03,  -9.22691683e-04,\n",
       "        -2.84510090e-04,  -1.64389646e-03,  -3.26945944e-04,\n",
       "        -3.92346478e-03,  -1.24094875e-03,  -2.08329691e-03,\n",
       "         3.50814454e-04,  -1.03091782e-03,  -1.26843367e-04,\n",
       "         6.97385036e-03,  -2.03436277e-03,   5.14895059e-03,\n",
       "         3.15651110e-04,  -9.40652166e-04,  -2.38359349e-03,\n",
       "         4.42236162e-04,  -2.04895858e-04,   2.18404662e-03,\n",
       "        -7.06268085e-04,   6.33044289e-03,  -6.29877955e-04,\n",
       "        -2.50667725e-02,   4.73314464e-03,  -8.05739030e-03,\n",
       "        -5.08215320e-04,   5.00475324e-03,  -2.78696199e-04,\n",
       "         5.64127957e-04,  -2.05451116e-03,   4.76081261e-04,\n",
       "        -2.92645468e-03,  -1.65315273e-03,  -1.67109293e-03,\n",
       "         6.13918435e-04,  -1.39612587e-03,   1.71390932e-04,\n",
       "         1.24857234e-02,  -5.21346531e-03,   8.29875815e-03,\n",
       "         6.41142500e-03,  -3.05580020e-03,  -2.68445460e-04,\n",
       "        -1.58229476e-03,  -3.43327251e-03,   5.06952397e-03,\n",
       "         1.45911931e-03,  -1.59830262e-03,   1.40118988e-03,\n",
       "        -4.06335747e-03,  -1.22538965e-03,  -2.09885602e-03,\n",
       "         2.53191488e-04,  -9.82797413e-04,   6.51195588e-04,\n",
       "        -1.03454259e-02,   7.87305034e-05,  -7.04907433e-03,\n",
       "        -6.11723994e-03,  -2.12701882e-03,  -1.19722684e-03,\n",
       "        -1.36330549e-03,  -4.34066944e-04,  -4.23806554e-03,\n",
       "         3.70855769e-03,  -4.51715561e-04,   4.10414692e-03,\n",
       "        -6.58195843e-03,  -1.93602040e-04,  -3.13064362e-03,\n",
       "        -1.76161703e-04,   9.02278890e-04,   2.40146823e-03,\n",
       "         2.44653753e-04,   1.29644716e-02,   1.63253032e-03,\n",
       "        -2.34812877e-04,  -2.57333815e-03,   1.46812154e-03,\n",
       "         4.48014779e-03,  -4.52610778e-03,   2.86443931e-04,\n",
       "         9.79743994e-04,   1.29943199e-02,   1.19296540e-03,\n",
       "        -8.13955443e-04,  -3.71623764e-03,  -2.05947394e-04,\n",
       "        -8.09745095e-03,  -1.51689844e-02,   2.08541680e-03])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Final_CrossValidation import *\n",
    "from implementations import *\n",
    "k_fold=4\n",
    "lamb=[0,0.5,1]\n",
    "degree_2 = 3\n",
    "gamma=0.0001\n",
    "max_iters=100\n",
    "\n",
    "current_y=np.ones(len(y2))\n",
    "current_y[np.where(y2==0)]=-1\n",
    "phi_X2=build_poly_cos_sin_poly(X2, degree_2, np.array(range(1,23)))\n",
    "#k_indices = build_k_indices(current_y, k_fold, 1)\n",
    "cross_validation_demo_ridge_regression(current_y,phi_X2,k_fold,lamb,seed=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=36421.46324341158\n",
      "Current iteration=100, the loss=30089.42686847765\n",
      "Current iteration=200, the loss=28331.034249438253\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-42cd7f7d8f91>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mtraining_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly_cos_sin_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mw_star\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# compute classification accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nadeen\\Desktop\\finalized_submission_ml\\ml-group78-submission\\Project_scripts\\implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, gamma, max_iters)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[1;31m# start the logistic regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[1;31m# get loss and updated w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\nadeen\\Desktop\\finalized_submission_ml\\ml-group78-submission\\Project_scripts\\implementations.py\u001b[0m in \u001b[0;36mcalculate_gradient_log_likelihood\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_gradient_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[1;34m\"\"\"compute the gradient of negative log likelihood.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;31m# stochastic gradient descent helpers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from implementations import *\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters_2 = 15000\n",
    "gamma_2 = 1e-7\n",
    "lambda_2 = 0.0\n",
    "degree_2 = 3\n",
    "batch_size2 = 100\n",
    "k = 4\n",
    "current_y = y2\n",
    "current_X = X2\n",
    "k_indices = build_k_indices(current_y, k, 1)\n",
    "\n",
    "loss_tr = []\n",
    "loss_te = []\n",
    "classification_acc = []\n",
    "for k_ in range(k):\n",
    "\n",
    "    test_indices = k_indices[k_]\n",
    "    test_y, test_x = (current_y[test_indices], current_X[test_indices])\n",
    "    test_x = build_poly_cos_sin_poly(test_x, degree_2, np.array(range(1,50)))\n",
    "    \n",
    "\n",
    "    training_indices = np.ravel(np.delete(k_indices, k_, axis=0))\n",
    "    training_y, training_x = (current_y[training_indices], current_X[training_indices])\n",
    "    training_x = build_poly_cos_sin_poly(training_x, degree_2, np.array(range(1,50)))\n",
    "    \n",
    "    w_star = reg_logistic_regression(training_y, training_x, lambda_2, gamma_2, max_iters_2)\n",
    "    \n",
    "    # compute classification accuracy\n",
    "    y_pred = predict_labels_log_regression(w_star, test_x)\n",
    "    \n",
    "    test_y[test_y == 0] = -1\n",
    "    classification_acc.append(np.mean(np.abs(test_y + y_pred) / 2))\n",
    "    test_y[test_y == -1] = 0\n",
    "\n",
    "    loss_tr.append(calculate_loss_log_likelihood(training_y, training_x, w_star))\n",
    "    loss_te.append(calculate_loss_log_likelihood(test_y, test_x, w_star))\n",
    "        \n",
    "print(\"Training error: {tr}\\nTest error: {te}\\nClassification accuracy: {cl}\".format(tr=np.mean(loss_tr), te=np.mean(loss_te), cl=np.mean(classification_acc)))\n",
    "overall_acc.append(np.mean(classification_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "print(\"The overall accuracy: {acc}\\n\".format(acc=np.mean(overall_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Best Degree Settings\n",
    "degree_0 = 2\n",
    "degree_1 = 3\n",
    "degree_2 = 3\n",
    "\n",
    "##Best parameter setting for Logistic Regression \n",
    "gamma_0 = 1e-6\n",
    "max_iters_0 = 15000\n",
    "\n",
    "##Not used since ridge regression was chosen for groups 1&2\n",
    "#max_iters_1 = 25000\n",
    "#gamma_1 = 0.9e-7\n",
    "#max_iters_2 = 15000\n",
    "#gamma_2 = 1e-7\n",
    "#max_iters_3 = 15000\n",
    "#gamma_3 = 1e-6\n",
    "\n",
    "##Best parameter setting for Ridge Regression\n",
    "lambda_1=[0]\n",
    "lambda_2=[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=55629.99811591573\n",
      "Current iteration=100, the loss=37732.06469358346\n",
      "Current iteration=200, the loss=37075.86407428958\n",
      "Current iteration=300, the loss=36664.45611615651\n",
      "Current iteration=400, the loss=36363.877903319\n",
      "Current iteration=500, the loss=36133.314997351044\n",
      "Current iteration=600, the loss=35950.737349300165\n",
      "Current iteration=700, the loss=35802.47535732489\n",
      "Current iteration=800, the loss=35679.60495531724\n",
      "Current iteration=900, the loss=35576.08045023632\n",
      "Current iteration=1000, the loss=35487.67060717105\n",
      "Current iteration=1100, the loss=35411.328558565176\n",
      "Current iteration=1200, the loss=35344.803573926525\n",
      "Current iteration=1300, the loss=35286.39335096982\n",
      "Current iteration=1400, the loss=35234.78183975122\n",
      "Current iteration=1500, the loss=35188.93100645239\n",
      "Current iteration=1600, the loss=35148.007360593365\n",
      "Current iteration=1700, the loss=35111.33119421142\n",
      "Current iteration=1800, the loss=35078.34083102715\n",
      "Current iteration=1900, the loss=35048.5669238233\n",
      "Current iteration=2000, the loss=35021.613582654776\n",
      "Current iteration=2100, the loss=34997.144229538986\n",
      "Current iteration=2200, the loss=34974.87078699593\n",
      "Current iteration=2300, the loss=34954.545265325134\n",
      "Current iteration=2400, the loss=34935.95311032753\n",
      "Current iteration=2500, the loss=34918.907868198396\n",
      "Current iteration=2600, the loss=34903.24685439519\n",
      "Current iteration=2700, the loss=34888.827601419805\n",
      "Current iteration=2800, the loss=34875.52492102387\n",
      "Current iteration=2900, the loss=34863.2284584134\n",
      "Current iteration=3000, the loss=34851.84064545111\n",
      "Current iteration=3100, the loss=34841.27498056891\n",
      "Current iteration=3200, the loss=34831.454577885896\n",
      "Current iteration=3300, the loss=34822.3109388715\n",
      "Current iteration=3400, the loss=34813.78290820139\n",
      "Current iteration=3500, the loss=34805.81578213177\n",
      "Current iteration=3600, the loss=34798.36054328772\n",
      "Current iteration=3700, the loss=34791.37320048231\n",
      "Current iteration=3800, the loss=34784.81421617\n",
      "Current iteration=3900, the loss=34778.648007453696\n",
      "Current iteration=4000, the loss=34772.84250926617\n",
      "Current iteration=4100, the loss=34767.368790508976\n",
      "Current iteration=4200, the loss=34762.200715639054\n",
      "Current iteration=4300, the loss=34757.314645530365\n",
      "Current iteration=4400, the loss=34752.689172486374\n",
      "Current iteration=4500, the loss=34748.30488510552\n",
      "Current iteration=4600, the loss=34744.14415935937\n",
      "Current iteration=4700, the loss=34740.190972773315\n",
      "Current iteration=4800, the loss=34736.43073903211\n",
      "Current iteration=4900, the loss=34732.85016068909\n",
      "Current iteration=5000, the loss=34729.437097953894\n",
      "Current iteration=5100, the loss=34726.18045178222\n",
      "Current iteration=5200, the loss=34723.07005969918\n",
      "Current iteration=5300, the loss=34720.09660296491\n",
      "Current iteration=5400, the loss=34717.251523842526\n",
      "Current iteration=5500, the loss=34714.52695185839\n",
      "Current iteration=5600, the loss=34711.91563805959\n",
      "Current iteration=5700, the loss=34709.410896374706\n",
      "Current iteration=5800, the loss=34707.00655127551\n",
      "Current iteration=5900, the loss=34704.69689102052\n",
      "Current iteration=6000, the loss=34702.476625837866\n",
      "Current iteration=6100, the loss=34700.34085047511\n",
      "Current iteration=6200, the loss=34698.28501060827\n",
      "Current iteration=6300, the loss=34696.3048726614\n",
      "Current iteration=6400, the loss=34694.39649664192\n",
      "Current iteration=6500, the loss=34692.55621164493\n",
      "Current iteration=6600, the loss=34690.78059372322\n",
      "Current iteration=6700, the loss=34689.066445858065\n",
      "Current iteration=6800, the loss=34687.41077979933\n",
      "Current iteration=6900, the loss=34685.81079957284\n",
      "Current iteration=7000, the loss=34684.263886478475\n",
      "Current iteration=7100, the loss=34682.767585424604\n",
      "Current iteration=7200, the loss=34681.31959246294\n",
      "Current iteration=7300, the loss=34679.91774340502\n",
      "Current iteration=7400, the loss=34678.56000341425\n",
      "Current iteration=7500, the loss=34677.24445748071\n",
      "Current iteration=7600, the loss=34675.96930169568\n",
      "Current iteration=7700, the loss=34674.732835251605\n",
      "Current iteration=7800, the loss=34673.5334531015\n",
      "Current iteration=7900, the loss=34672.36963921819\n",
      "Current iteration=8000, the loss=34671.23996040004\n",
      "Current iteration=8100, the loss=34670.143060574585\n",
      "Current iteration=8200, the loss=34669.077655556786\n",
      "Current iteration=8300, the loss=34668.042528222024\n",
      "Current iteration=8400, the loss=34667.0365240582\n",
      "Current iteration=8500, the loss=34666.058547064225\n",
      "Current iteration=8600, the loss=34665.10755596549\n",
      "Current iteration=8700, the loss=34664.18256071917\n",
      "Current iteration=8800, the loss=34663.28261928493\n",
      "Current iteration=8900, the loss=34662.40683463881\n",
      "Current iteration=9000, the loss=34661.55435200971\n",
      "Current iteration=9100, the loss=34660.7243563202\n",
      "Current iteration=9200, the loss=34659.916069814455\n",
      "Current iteration=9300, the loss=34659.12874985818\n",
      "Current iteration=9400, the loss=34658.361686896176\n",
      "Current iteration=9500, the loss=34657.61420255484\n",
      "Current iteration=9600, the loss=34656.885647878014\n",
      "Current iteration=9700, the loss=34656.17540168513\n",
      "Current iteration=9800, the loss=34655.48286904219\n",
      "Current iteration=9900, the loss=34654.807479836505\n",
      "Current iteration=10000, the loss=34654.14868744707\n",
      "Current iteration=10100, the loss=34653.50596750304\n",
      "Current iteration=10200, the loss=34652.87881672376\n",
      "Current iteration=10300, the loss=34652.26675183357\n",
      "Current iteration=10400, the loss=34651.669308546334\n",
      "Current iteration=10500, the loss=34651.086040613875\n",
      "Current iteration=10600, the loss=34650.516518933975\n",
      "Current iteration=10700, the loss=34649.960330713235\n",
      "Current iteration=10800, the loss=34649.417078680905\n",
      "Current iteration=10900, the loss=34648.88638035003\n",
      "Current iteration=11000, the loss=34648.367867322384\n",
      "Current iteration=11100, the loss=34647.86118463425\n",
      "Current iteration=11200, the loss=34647.36599013995\n",
      "Current iteration=11300, the loss=34646.881953930744\n",
      "Current iteration=11400, the loss=34646.40875778655\n",
      "Current iteration=11500, the loss=34645.946094658124\n",
      "Current iteration=11600, the loss=34645.49366817793\n",
      "Current iteration=11700, the loss=34645.05119219751\n",
      "Current iteration=11800, the loss=34644.618390349904\n",
      "Current iteration=11900, the loss=34644.19499563507\n",
      "Current iteration=12000, the loss=34643.780750027334\n",
      "Current iteration=12100, the loss=34643.37540410303\n",
      "Current iteration=12200, the loss=34642.97871668736\n",
      "Current iteration=12300, the loss=34642.59045451903\n",
      "Current iteration=12400, the loss=34642.21039193186\n",
      "Current iteration=12500, the loss=34641.838310552\n",
      "Current iteration=12600, the loss=34641.47399901014\n",
      "Current iteration=12700, the loss=34641.117252667565\n",
      "Current iteration=12800, the loss=34640.76787335539\n",
      "Current iteration=12900, the loss=34640.42566912617\n",
      "Current iteration=13000, the loss=34640.09045401716\n",
      "Current iteration=13100, the loss=34639.76204782456\n",
      "Current iteration=13200, the loss=34639.44027588816\n",
      "Current iteration=13300, the loss=34639.124968885815\n",
      "Current iteration=13400, the loss=34638.81596263709\n",
      "Current iteration=13500, the loss=34638.513097915755\n",
      "Current iteration=13600, the loss=34638.21622027058\n",
      "Current iteration=13700, the loss=34637.925179853875\n",
      "Current iteration=13800, the loss=34637.639831257504\n",
      "Current iteration=13900, the loss=34637.36003335611\n",
      "Current iteration=14000, the loss=34637.0856491567\n",
      "Current iteration=14100, the loss=34636.81654565486\n",
      "Current iteration=14200, the loss=34636.552593696826\n",
      "Current iteration=14300, the loss=34636.29366784737\n",
      "Current iteration=14400, the loss=34636.03964626306\n",
      "Current iteration=14500, the loss=34635.79041057066\n",
      "Current iteration=14600, the loss=34635.54584575055\n",
      "Current iteration=14700, the loss=34635.30584002476\n",
      "Current iteration=14800, the loss=34635.07028474942\n",
      "Current iteration=14900, the loss=34634.839074311545\n",
      "The loss=34634.61435504408\n",
      "The loss=0.28310022968560256\n",
      "The loss=0.2830360336727703\n",
      "The loss=0.2812760401972303\n",
      "The loss=0.2835648428878991\n",
      "MSEtr=[ 0.28274429],MSEte=[ 0.28436645], MSEvartr=[  7.60217989e-07], MSEvarte=[  7.13341221e-06], Accuracy=[ 0.80652275]\n",
      "The loss=0.2544387314752559\n",
      "The loss=0.25475771278135806\n",
      "The loss=0.256355747722205\n",
      "The loss=0.25509932170467936\n",
      "MSEtr=[ 0.25516288],MSEte=[ 0.25739929], MSEvartr=[  5.28881155e-07], MSEvarte=[  4.22513951e-06], Accuracy=[ 0.83296112]\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import *\n",
    "from implementations import *\n",
    "from Final_CrossValidation import *\n",
    "\n",
    "X0, X1, X2, indices0, indices1, indices2 = separate_data(X)\n",
    "\n",
    "##Return y vector back to [-1,1] to be used with ridge regression\n",
    "#y0 = y[indices0]\n",
    "#y_s0=np.ones(len(y0))\n",
    "#y_s0[np.where(y0==0)]=-1\n",
    "\n",
    "y1 = y[indices1]\n",
    "y_s1=np.ones(len(y1))\n",
    "y_s1[np.where(y1==0)]=-1\n",
    "y2 = y[indices2]\n",
    "y_s2=np.ones(len(y2))\n",
    "y_s2[np.where(y2==0)]=-1\n",
    "#y3 = y[indices3]\n",
    "\n",
    "\n",
    "phi_X0 = build_poly_cos_sin_poly(X0, degree_0, np.array(range(1,19)))\n",
    "phi_X1 = build_poly_cos_sin_poly(X1, degree_1, np.array(range(1,23)))\n",
    "phi_X2 = build_poly_cos_sin_poly(X2, degree_2, np.array(range(1,30)))\n",
    "#phi_X3 = build_poly(X3, degree_3)\n",
    "\n",
    "w0 = reg_logistic_regression(y0, phi_X0, 0, gamma_0, max_iters_0)[0]\n",
    "#w1 = reg_logistic_regression(y1, phi_X1, 0, gamma_1, max_iters_1)\n",
    "#w2 = reg_logistic_regression(y2, phi_X2, 0, gamma_2, max_iters_2)\n",
    "#w3 = reg_logistic_regression(y3, phi_X3, 0, gamma_3, max_iters_3)\n",
    "\n",
    "#w0=cross_validation_demo_ridge_regression(y_s0,phi_X0,4,[0.01],seed=250)\n",
    "w1=cross_validation_demo_ridge_regression(y_s1,phi_X1,4,[0],seed=250)\n",
    "w2=cross_validation_demo_ridge_regression(y_s2,phi_X2,4,[0],seed=250)\n",
    "#print(w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "X0_te, X1_te, X2_te, indices0_te, indices1_te, indices2_te = separate_data(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../woohoo.csv' # TODO: fill in desired name of output file for submission\n",
    "ids_test0 = ids_test[indices0_te]\n",
    "ids_test1 = ids_test[indices1_te]\n",
    "ids_test2 = ids_test[indices2_te]\n",
    "#ids_test3 = ids_test[indices3_te]\n",
    "\n",
    "phi_X0_te = build_poly_cos_sin_poly(X0_te, degree_0, np.array(range(1,19)))\n",
    "phi_X1_te = build_poly_cos_sin_poly(X1_te, degree_1, np.array(range(1,23)))\n",
    "phi_X2_te = build_poly_cos_sin_poly(X2_te, degree_2, np.array(range(1,30)))\n",
    "#phi_X3_te = build_poly(X3_te, degree_3)\n",
    "\n",
    "\n",
    "pred0 = predict_labels_log_regression(w0, phi_X0_te)\n",
    "#pred1 = predict_labels_log_regression(w1, phi_X1_te)\n",
    "#pred2 = predict_labels_log_regression(w2, phi_X2_te)\n",
    "#pred3 = predict_labels_log_regression(w3, phi_X3_te)\n",
    "#pred0 = predict_labels(w0, phi_X0_te)\n",
    "pred1 = predict_labels(w1, phi_X1_te)\n",
    "pred2 = predict_labels(w2, phi_X2_te)\n",
    "\n",
    "predictions = np.append(pred0, np.append(pred1, pred2))\n",
    "ids = np.append(ids_test0, np.append(ids_test1, ids_test2))\n",
    "\n",
    "create_csv_submission(ids, predictions, OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
